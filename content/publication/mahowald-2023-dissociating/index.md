---
title: 'Dissociating language and thought in large language models: a cognitive perspective'
date: '2023-01-01'
publishDate: '2022-09-17T21:47:51.944591Z'
authors:
- Kyle Mahowald
- admin
- Idan A Blank
- Nancy Kanwisher
- Josh Tenenbaum
- Evelina Fedorenko

# Author notes (optional)
author_notes:
  - 'Equal contribution'
  - 'Equal contribution'

publication: '*arXiv*'
publication_types:
- '3'

abstract: ''

summary: 'Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their capabilities remain split. Here, we evaluate LLMs using a distinction between formal competence — knowledge of linguistic rules and patterns — and functional competence — understanding and using language in the world. We ground this distinction in human neuroscience, showing that these skills recruit different cognitive mechanisms. Although LLMs are close to mastering formal competence, they still fail at functional competence tasks, which often require drawing on non-linguistic capacities. In short, LLMs are good models of language but incomplete models of human thought.'

featured: true

# Focal point (optional)
# Options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
image:
  caption: 'Language understanding requires not only language-specific, but also general cognitive abilities.'
  focal_point: 'Smart'
  preview_only: false

tags:
- LLMs
- review
- position paper

url_pdf: https://arxiv.org/pdf/2301.06627.pdf
doi: https://doi.org/10.48550/arXiv.2301.06627


links:
- name: Tweeprint
  url: https://twitter.com/neuranna/status/1615737072207400962
---
